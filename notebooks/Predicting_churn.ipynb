{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, f1_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from datetime import timedelta\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/transactions_dataset.csv', sep=';')\n",
    "clients = pd.read_csv('data/sales_client_relationship_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data = df.isnull().sum()\n",
    "\n",
    "print(\"Missing values per column:\\n\", missing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'date_order' and 'date_invoice' to datetime\n",
    "df['date_order'] = pd.to_datetime(df['date_order'])\n",
    "df['date_invoice'] = pd.to_datetime(df['date_invoice'])\n",
    "\n",
    "df['order_channel'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('number of clients in the dataset', len(df['client_id'].unique()))\n",
    "print('number of stores in the dataset', len(df['branch_id'].unique()))\n",
    "print('number of type of products in the dataset', len(df['product_id'].unique()))\n",
    "print('number of channel types in the dataset', len(df['order_channel'].unique()))\n",
    "print('number of transactions in the dataset', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for channel in df['order_channel'].unique() :\n",
    "    # Filter the dataframe for the current channel\n",
    "    df_channel = df[df['order_channel'] == channel]\n",
    "\n",
    "    # Count unique stores in the filtered dataframe\n",
    "    unique_stores = df_channel['branch_id'].nunique()\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Channel: {channel}\")\n",
    "    print(f\"revenues per year: {unique_stores}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data cleaning and transformaton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the delivery time of orders\n",
    "df['days_to_deliver'] = (df['date_invoice'] - df['date_order']).dt.days\n",
    "df = df[df.days_to_deliver > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"there are {df[df.sales_net == 0].shape[0]} rows with sales = 0\")\n",
    "df = df[df.sales_net > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Transform dataset so that each row is a whole purchase order\n",
    "df_grouped = df.groupby([\"client_id\", \"date_order\"]).agg(\n",
    "    products_sold_quantities=(\"product_id\", lambda x: dict(zip(x, df.loc[x.index, 'quantity']))),  # Dictionary of product_id: quantity\n",
    "    branches=(\"branch_id\", lambda x: list(set(x))),  # Unique branches\n",
    "    sales_channels=(\"order_channel\", lambda x: list(set(x))),  # Unique sales channels\n",
    "    revenue=('sales_net', sum)\n",
    ").reset_index()\n",
    "\n",
    "df_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Add features\n",
    "df_grouped = df_grouped.sort_values(by=['client_id', 'date_order'])\n",
    "df_grouped['time_since_last_sale'] = df_grouped.groupby('client_id')['date_order'].diff().dt.days.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove outliers and clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Handle outliers (lenient approach)\n",
    "Q1 = df_grouped['time_since_last_sale'].quantile(0.25)\n",
    "Q3 = df_grouped['time_since_last_sale'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define the upper bound for outliers\n",
    "upper_bound = Q3 + 15 * IQR\n",
    "\n",
    "# Filter the dataframe to remove outliers above the upper bound\n",
    "df_grouped_filtered = df_grouped[df_grouped['time_since_last_sale'] <= upper_bound]\n",
    "\n",
    "# Print the number of removed outliers\n",
    "num_outliers = df_grouped.shape[0] - df_grouped_filtered.shape[0]\n",
    "print(f\"Number of outliers removed: {num_outliers}\")\n",
    "print(f\"Which represents {num_outliers / df_grouped.shape[0] * 100:.2f}% of the data points\")\n",
    "print(f\"Upper Bound is {upper_bound}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Extract the features for clustering\n",
    "df_grouped['days_since_last_sale'] = df_grouped['time_since_last_sale']\n",
    "client_features = df_grouped.groupby('client_id')['days_since_last_sale'].mean().reset_index()\n",
    "features = client_features[['days_since_last_sale']].values\n",
    "\n",
    "# Standard scale the features\n",
    "scaler = StandardScaler()\n",
    "features = scaler.fit_transform(features)\n",
    "\n",
    "# Determine the optimal number of clusters using the Elbow Method and Silhouette Score\n",
    "inertia = []\n",
    "silhouette_scores = []\n",
    "K = range(1, 11)\n",
    "\n",
    "for k in K:\n",
    "    print(k)\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "    kmeans.fit(features)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "    if k > 1:\n",
    "        silhouette_scores.append(silhouette_score(features, kmeans.labels_))\n",
    "\n",
    "# Plot the Elbow Method\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(K, inertia, 'bx-')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()\n",
    "\n",
    "# Plot the Silhouette Score\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(K[1:], silhouette_scores, 'bx-')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Perform clustering\n",
    "# StandardScaler for scaling\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(df_grouped_filtered[['time_since_last_sale']])\n",
    "\n",
    "# KMeans clustering\n",
    "optimal_k = 4  # Based on prior analysis\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=0)\n",
    "df_grouped_filtered['client_cluster'] = kmeans.fit_predict(scaled_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by cluster and calculate the number of people in each cluster\n",
    "cluster_counts = df_grouped_filtered['client_cluster'].value_counts().sort_index()\n",
    "\n",
    "# Calculate the average, minimum, and maximum time_since_last_sale for each cluster\n",
    "cluster_stats = df_grouped_filtered.groupby('client_cluster').agg(\n",
    "    mean_time_since_last_sale=('time_since_last_sale', 'mean'),\n",
    "    min_time_since_last_sale=('time_since_last_sale', 'min'),\n",
    "    max_time_since_last_sale=('time_since_last_sale', 'max'),\n",
    "    number_of_people=('client_id', 'nunique')\n",
    ")\n",
    "cluster_stats[['mean_time_since_last_sale', 'number_of_people']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map clusters to periodicity (based on domain knowledge)\n",
    "mapping = {\n",
    "    0: 5,   # Weekly\n",
    "    1: 30,  # Monthly\n",
    "    2: 75,  # Quarterly\n",
    "    3: 150  # Bi-yearly\n",
    "}\n",
    "df_grouped_filtered['periodicity'] = df_grouped_filtered['client_cluster'].map(mapping)\n",
    "\n",
    "df_grouped_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# defining churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Define churn based on cluster-specific periodicity\n",
    "df_grouped_filtered['expected_next_purchase'] = df_grouped_filtered['date_order'] + pd.to_timedelta(df_grouped_filtered['periodicity'], unit='D')\n",
    "df_grouped_filtered['churn'] = (df_grouped_filtered['expected_next_purchase'] < df_grouped_filtered['date_order'].max()).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Feature engineering\n",
    "# Aggregate client-level features\n",
    "client_features = df_grouped_filtered.groupby('client_id').agg({\n",
    "    'time_since_last_sale': 'mean',\n",
    "    'revenue': 'sum',\n",
    "    'products_sold_quantities': 'count',  # Number of orders\n",
    "    'branches': lambda x: len(set([item for sublist in x for item in sublist])),  # Unique branches\n",
    "    'sales_channels': lambda x: len(set([item for sublist in x for item in sublist])),  # Unique sales channels\n",
    "    'churn': 'max'  # Target variable\n",
    "}).reset_index()\n",
    "\n",
    "#Rename columns for clarity\n",
    "client_features.rename(columns={\n",
    "    'time_since_last_sale': 'avg_time_since_last_sale',\n",
    "    'revenue': 'total_revenue',\n",
    "    'products_sold_quantities': 'total_orders',\n",
    "    'branches': 'unique_branches',\n",
    "    'sales_channels': 'unique_sales_channels'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped_filtered = pd.merge(df_grouped_filtered, clients, on='client_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define the time range for rolling windows\n",
    "# Get the minimum and maximum order dates from your dataset\n",
    "min_order_date = df_grouped_filtered['date_order'].min()\n",
    "max_order_date = df_grouped_filtered['date_order'].max()\n",
    "\n",
    "# Define the start and end dates for rolling windows\n",
    "# Example: Use the last 12 months for evaluation, and start 6 months before that\n",
    "end_date = max_order_date - pd.DateOffset(months=12)  # Last 12 months for testing\n",
    "start_date = end_date - pd.DateOffset(months=6)       # 6 months before for training\n",
    "\n",
    "# Step 2: Generate monthly points in time\n",
    "points_in_time = pd.date_range(start=start_date, end=end_date, freq='MS')  # Monthly intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Step 1: One-Hot Encode categorical columns\n",
    "def one_hot_encode_categorical(df, categorical_columns):\n",
    "    \"\"\"One-hot encode specified categorical columns.\"\"\"\n",
    "    encoded_dfs = []\n",
    "    \n",
    "    for col in categorical_columns:\n",
    "        if isinstance(df[col].iloc[0], list):  # Check if the column contains lists\n",
    "            mlb = MultiLabelBinarizer()\n",
    "            encoded_data = mlb.fit_transform(df[col])\n",
    "            encoded_df = pd.DataFrame(encoded_data, columns=[f\"{col}_{channel}\" for channel in mlb.classes_])\n",
    "        else:  # Regular categorical column\n",
    "            encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "            encoded_data = encoder.fit_transform(df[[col]])\n",
    "            encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out([col]))\n",
    "        \n",
    "        encoded_dfs.append(encoded_df)\n",
    "    \n",
    "    # Combine encoded data with the original dataframe\n",
    "    return pd.concat([df.drop(columns=categorical_columns)] + encoded_dfs, axis=1)\n",
    "\n",
    "# List of categorical columns to encode\n",
    "categorical_columns = ['quali_relation', 'sales_channels']\n",
    "\n",
    "# Apply one-hot encoding to the main dataset\n",
    "df_grouped_filtered_encoded = one_hot_encode_categorical(df_grouped_filtered, categorical_columns)\n",
    "\n",
    "# Step 2: Update the dataset creation function to include encoded features\n",
    "def create_datasets_with_periodicity(date, periodicity_multiplier=2):\n",
    "    # Get periodicity from historical clustering\n",
    "    periodicity_df = cluster_clients_historical(df_grouped_filtered_encoded, date)\n",
    "    \n",
    "    # Filter data up to the given date\n",
    "    historical_data = df_grouped_filtered_encoded[df_grouped_filtered_encoded['date_order'] < date]\n",
    "    \n",
    "    # Aggregate features without future knowledge\n",
    "    features = historical_data.groupby('client_id').agg(\n",
    "        recency=('date_order', lambda x: (date - x.max()).days),\n",
    "        frequency=('date_order', 'count'),\n",
    "        total_revenue=('revenue', 'sum'),\n",
    "        unique_branches=('branches', lambda x: x.explode().nunique()),\n",
    "        **{col: (col, 'sum') for col in df_grouped_filtered_encoded.columns if col.startswith('sales_channels_')},\n",
    "        **{col: (col, 'sum') for col in df_grouped_filtered_encoded.columns if col.startswith('quali_relation_')}\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Merge periodicity from historical clustering\n",
    "    features = pd.merge(features, periodicity_df, on='client_id', how='left')\n",
    "    \n",
    "    # Define churn: no purchase in periodicity * multiplier days after last date\n",
    "    features['churn'] = (features['recency'] > features['periodicity'] * periodicity_multiplier).astype(int)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Step 3: Rebuild datasets with temporal integrity\n",
    "all_datasets = []\n",
    "for date in points_in_time:\n",
    "    dataset = create_datasets_with_periodicity(date, periodicity_multiplier=4)\n",
    "    all_datasets.append(dataset)\n",
    "combined_dataset = pd.concat(all_datasets, ignore_index=True)\n",
    "\n",
    "# Step 4: Time-based train-test split\n",
    "combined_dataset['max_date'] = pd.to_datetime(points_in_time[-1]) - pd.to_timedelta(combined_dataset['recency'], unit='D')\n",
    "combined_dataset.sort_values('max_date', inplace=True)\n",
    "\n",
    "# Split 80-20 temporally\n",
    "split_idx = int(len(combined_dataset) * 0.8)\n",
    "X_train = combined_dataset.iloc[:split_idx].drop(columns=['client_id', 'churn', 'max_date'])\n",
    "y_train = combined_dataset.iloc[:split_idx]['churn']\n",
    "X_test = combined_dataset.iloc[split_idx:].drop(columns=['client_id', 'churn', 'max_date'])\n",
    "y_test = combined_dataset.iloc[split_idx:]['churn']\n",
    "\n",
    "# Step 5: Handle class imbalance (SMOTE instead of undersampling)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Step 6: Train and evaluate\n",
    "xgb = XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "xgb.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Evaluation\n",
    "y_pred = xgb.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"AUC: {roc_auc_score(y_test, xgb.predict_proba(X_test)[:,1])}\")\n",
    "\n",
    "# Step 7: Predict churn probabilities for non-churned clients\n",
    "current_date = df_grouped_filtered_encoded['date_order'].max()\n",
    "current_data = create_datasets_with_periodicity(current_date, periodicity_multiplier=4)\n",
    "non_churned = current_data[current_data['churn'] == 0].copy()\n",
    "\n",
    "# Prepare prediction data (same features as training)\n",
    "X_live = non_churned.drop(columns=['client_id', 'churn'])\n",
    "\n",
    "# Generate predictions\n",
    "non_churned['churn_probability'] = xgb.predict_proba(X_live)[:, 1]\n",
    "\n",
    "# Sort and format results\n",
    "churn_risk_list = (non_churned[['client_id', 'churn_probability']]\n",
    "                   .sort_values('churn_probability', ascending=False)\n",
    "                   .reset_index(drop=True))\n",
    "\n",
    "print(\"Top 10 At-Risk Clients:\")\n",
    "print(churn_risk_list.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "y_pred = xgb.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"AUC: {roc_auc_score(y_test, xgb.predict_proba(X_test)[:,1])})\")\n",
    "\n",
    "# Confusion Matrix\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=confusion)\n",
    "disp.plot()\n",
    "\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Proportion of churn in dataset {y.mean()*100:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Optional: Save results\n",
    "churn_risk_list.to_csv('client_churn_risk_ranking.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_risk_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
